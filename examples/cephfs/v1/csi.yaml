apiVersion: storage.tkestack.io/v1
kind: CSI
metadata:
  name: cephfs
  namespace: kube-system
spec:
  driverName: csi-cephfs
  driverTemplate:
    template:
      spec:
        hostNetwork: true
        # to use e.g. Rook orchestrated cluster, and mons' FQDN is
        # resolved through k8s service, set dns policy to cluster first
        dnsPolicy: ClusterFirstWithHostNet
        containers:
        - name: csi-cephfs
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
            allowPrivilegeEscalation: true
          image: quay.io/cephcsi/cephfsplugin:v1.0.0
          args:
            - "--nodeid=$(NODE_ID)"
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--v=5"
            - "--drivername=csi-cephfs"
            - "--metadatastorage=k8s_configmap"
            - "--mountcachedir=/mount-cache-dir"
          env:
            - name: NODE_ID
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          imagePullPolicy: "IfNotPresent"
          volumeMounts:
            - name: mount-cache-dir
              mountPath: /mount-cache-dir
            - name: host-sys
              mountPath: /sys
            - name: lib-modules
              mountPath: /lib/modules
              readOnly: true
            - name: host-dev
              mountPath: /dev
        volumes:
        - name: mount-cache-dir
          emptyDir: {}
        - name: host-sys
          hostPath:
            path: /sys
        - name: lib-modules
          hostPath:
            path: /lib/modules
        - name: host-dev
          hostPath:
            path: /dev
  node:
    nodeRegistrar:
      image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0
    livenessProbe:
      image: quay.io/k8scsi/livenessprobe:v1.1.0
      parameters:
        storage.tkestack.io/liveness-probe-port: "9819"
  controller:
    replicas: 1
    provisioner:
      image: quay.io/k8scsi/csi-provisioner:v1.0.1
      resources:
        limits:
          "cpu": 100m
          "memory": 100Mi
    attacher:
      image: quay.io/k8scsi/csi-attacher:v1.1.0
      resources:
        limits:
          "cpu": 100m
          "memory": 100Mi
    clusterRegistrar:
      image: quay.io/k8scsi/csi-cluster-driver-registrar:canary
    livenessProbe:
      image: quay.io/k8scsi/livenessprobe:v1.1.0
      parameters:
        storage.tkestack.io/liveness-probe-port: "9818"
  secrets:
  - apiVersion: v1
    kind: Secret
    metadata:
      name: csi-cephfs-secret
      namespace: kube-system
    data:
      adminID: YWRtaW4=
      adminKey: QVFEMEpkRmNZVGFJT3hBQTNQQzhVeFdEY0Jib2JGQnp3aEMwcnc9PQ==
  storageClasses:
  - apiVersion: storage.k8s.io/v1
    kind: StorageClass
    metadata:
       name: csi-cephfs
    provisioner: csi-cephfs
    parameters:
        provisionVolume: "true"
        monitors: 10.0.0.213:6789,10.0.0.213:3300
        pool: myfs-data0
        csi.storage.k8s.io/provisioner-secret-name: csi-cephfs-secret
        csi.storage.k8s.io/provisioner-secret-namespace: kube-system
        csi.storage.k8s.io/node-stage-secret-name: csi-cephfs-secret
        csi.storage.k8s.io/node-stage-secret-namespace: kube-system
        adminid: admin
        userid: admin
    reclaimPolicy: Delete